{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "doAgp1etpKLh",
        "GpKMiHBHwxJf"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarioMameli/DL_Assignments/blob/main/DL_Assignment3_Clasification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL Assignment 3\n",
        "## Group 4\n",
        "**AUTHORS**:\n",
        "*   Dario Mameli [dario.mameli@ugent.be]\n",
        "*   Àngel Masip LLopis [angel.masipllopis@ugent.be]\n",
        "*   Michele Russo [michele.russo@ugent.be]\n",
        "\n",
        "\n",
        "\n",
        "This notebook is to be intended as both report and code.\n",
        "\n",
        "Running on google colab is suggested to make sure all required libraries and packages are present."
      ],
      "metadata": {
        "id": "FRPrCmqA3ZTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GitHub\n",
        "\n",
        "Let's clone the repo with the dataset"
      ],
      "metadata": {
        "id": "MIZ_-9-u6E7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dkdk22/DL_Ass3\n",
        "import sys\n",
        "# Add the repository to the path\n",
        "sys.path.insert(1, '/content/DL_Ass3/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz0pecYrli1O",
        "outputId": "306bc566-9496-4ad4-e7ca-3a49288f47fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DL_Ass3'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (3/3), 22.54 MiB | 19.95 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzIrHK0HnwaT"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Firstly let's setup the workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Installs"
      ],
      "metadata": {
        "id": "VxmIWEgYIL00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of TensorFlow, which includes Keras (tf.keras)\n",
        "#!pip install tensorflow --upgrade\n",
        "!pip uninstall keras\n",
        "!pip install keras --upgrade --quiet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install PrettyTable"
      ],
      "metadata": {
        "id": "gRSbKIjfILkE",
        "outputId": "2469eb2f-2180-47d6-911f-f2463b5a444d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.15.0\n",
            "Uninstalling keras-2.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/keras-2.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/keras/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled keras-2.15.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.8/286.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.10.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Requirement already satisfied: PrettyTable in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from PrettyTable) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiSBqqDaizpT"
      },
      "source": [
        "## Packages\n",
        "\n",
        "Let's import all the necessary functions and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_tndfUaBvYFN"
      },
      "outputs": [],
      "source": [
        "from dl_utils import plot_history\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "from keras import regularizers\n",
        "import matplotlib.gridspec as gridspec\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import keras_tuner as kt\n",
        "from keras import backend as backend\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Conv2DTranspose,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    GlobalAveragePooling2D,\n",
        "    MaxPooling2D,\n",
        "    UpSampling2D\n",
        ")\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras.applications as app\n",
        "from keras.layers import LSTM, Dense, Embedding, Input, GRU, Bidirectional\n",
        "\n",
        "import gc\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import zipfile\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Global variables"
      ],
      "metadata": {
        "id": "zg_oAPilI2CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED =42\n",
        "TRAIN_BASELINE = True\n",
        "TRAIN_MODEL = True"
      ],
      "metadata": {
        "id": "PCYSJedrI4nW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTb0qM5n7gE"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwus8LWjP33"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'DL_Ass3/review_553850.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: #Extract the file\n",
        "    zip_ref.extractall()\n",
        "\n",
        "with open('review_553850.json', 'r') as json_file: # Open the json file\n",
        "    data = json.load(json_file)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for id, review in data[\"reviews\"].items():\n",
        "    review_text = review[\"review\"]\n",
        "    funny_votes = review[\"votes_funny\"]\n",
        "    X.append(review_text)\n",
        "    y.append(funny_votes)"
      ],
      "metadata": {
        "id": "pjelOlrluxfZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "dGl0DJoAmnus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to do the same preprocessing as in the other notebook. We are going to eliminate stopwords and also we are going to use steeming, tokenize and pad the sentences.\n"
      ],
      "metadata": {
        "id": "-NEwlSBGwIrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "# Remove stopwords, convert to lowercase and stemming\n",
        "X_prepro = []\n",
        "for text in X:\n",
        "    words = text.split()\n",
        "\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_words]\n",
        "    preprocessed_text = ' '.join(stemmed_tokens)\n",
        "    X_prepro.append(preprocessed_text)\n",
        "# Remove the reviews with 0 words\n",
        "X_filtered = []\n",
        "y_filtered = []\n",
        "for review, label in zip(X_prepro, y):\n",
        "    if len(review.split()) > 0:\n",
        "        X_filtered.append(review)\n",
        "        y_filtered.append(label)\n",
        "print(\"Length of the dataset: \"+str(len(X_filtered)))\n",
        "# Tokenize\n",
        "num_words=10000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_filtered)\n",
        "X_sequences = tokenizer.texts_to_sequences(X_filtered)\n",
        "# Initialize variables to store information about the longest sequence\n",
        "max_length = 0\n",
        "max_index = -1\n",
        "\n",
        "# Iterate through the list of sequences to have the longes sequence\n",
        "for i, sequence in enumerate(X_sequences):\n",
        "    length = len(sequence)\n",
        "    # Update max_length and max_index if the current sequence is longer\n",
        "    if length > max_length:\n",
        "        max_length = length\n",
        "        max_index = i\n",
        "\n",
        "# Retrieve the longest sequence\n",
        "longest_sequence = X_sequences[max_index]\n",
        "\n",
        "# Print the longest sequence\n",
        "print(\"The longest sequence:\")\n",
        "print(longest_sequence)\n",
        "print(len(longest_sequence))\n",
        "# Padding\n",
        "max_sequence_length = len(longest_sequence)\n",
        "padded_sequences = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
        "# Convert to np arrays\n",
        "X = np.array(padded_sequences)\n",
        "y = np.array(y_filtered)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KO92gjnn0sL",
        "outputId": "e32539a0-d4a7-4721-8947-79438a6f26fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the dataset: 188843\n",
            "The longest sequence:\n",
            "[169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154, 169, 154]\n",
            "1600\n",
            "(188843, 1600)\n",
            "(188843,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary classification problem"
      ],
      "metadata": {
        "id": "6GLrF8Ncrkik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this is a really difficoult task to learn because most of the reviews have 0 funny_votes and some of them have a really high value of funny_votes. That's why we are going to change the approach and convert the problem to a binary classification problem, that should be easier to learn to our model."
      ],
      "metadata": {
        "id": "avBowkvxrqPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to convert it by fixing an number of funny_votes N, treating a review as funny or not funny depending on whether it has more or fewer votes than N. In our case N is going to be 1 because of the unbalancement of the dataset."
      ],
      "metadata": {
        "id": "R0df2-SXsKod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [0 if num == 0 else 1 for num in y]"
      ],
      "metadata": {
        "id": "WjWCPIAcQc1q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Model"
      ],
      "metadata": {
        "id": "ATIPmZsZNe9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "6madG_8ENiPE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Best model for classification problem"
      ],
      "metadata": {
        "id": "zLZVRqSVBapQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperModelBuilderBinary(kt.HyperModel):\n",
        "    def __init__(self, max_sequence_length, loss_function, max_features):\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.loss_function = loss_function\n",
        "        self.max_features = max_features\n",
        "\n",
        "    def build_lstm_model(self, hp):\n",
        "        model = Sequential()\n",
        "        print(\"seq\")\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))  #fix embedding size?\n",
        "        print(\"emb\")\n",
        "        model.add(LSTM(units=hp.Int('units', min_value=32, max_value=256, step=32), #https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "                       dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                       recurrent_dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1), #reduce two dropouts\n",
        "                       input_shape=(self.max_sequence_length, 1)))\n",
        "        print(\"lstm\")\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "        print(\"dense\")\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        print(\"compiled\")\n",
        "        return model\n",
        "\n",
        "    def build_bidirectional_lstm_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))\n",
        "        model.add(Bidirectional(LSTM(units=hp.Int('units', min_value=32, max_value=256, step=32),\n",
        "                                     dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                                     recurrent_dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                                     input_shape=(self.max_sequence_length, 1))))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_gru_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))\n",
        "        model.add(GRU(units=hp.Int('units', min_value=32, max_value=256, step=32),   #https://keras.io/api/layers/recurrent_layers/gru/\n",
        "                      dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                      recurrent_dropout=hp.Float('dropout', min_value=0.0, max_value=0.3, step=0.1),\n",
        "                      input_shape=(self.max_sequence_length, 1)))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def build_model(self,hp):\n",
        "        model_name = hp.Choice('model_name', values=['GRU'])\n",
        "        if model_name == 'LSTM':\n",
        "            return self.build_lstm_model(hp)\n",
        "        elif model_name == 'BidirectionalLSTM':\n",
        "            return self.build_bidirectional_lstm_model(hp)\n",
        "        elif model_name == 'GRU':\n",
        "            return self.build_gru_model(hp)\n",
        "\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        return model.fit(X_train, y_train, validation_data=(X_val, y_val))\n",
        "\n",
        "    def summary(self):\n",
        "        return self.model.summary()\n",
        "\n",
        "    def save(self, path):\n",
        "        return self.model.save(path)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.model.predict(x)"
      ],
      "metadata": {
        "id": "UeCOUYOMshVU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
        "    apply_class_balancing=False,\n",
        "    alpha=0.25,\n",
        "    gamma=2.0,\n",
        "    from_logits=False,\n",
        "    label_smoothing=0.0,\n",
        "    axis=-1,\n",
        "    name='binary_focal_crossentropy'\n",
        ")\n",
        "# Define hyperparameters for the tuner\n",
        "hypermodel_builder = HyperModelBuilderBinary(max_sequence_length=max_sequence_length,\n",
        "                                       loss_function=loss, max_features=10000)\n",
        "\n",
        "# Initialize tuner\n",
        "tuner = kt.BayesianOptimization(hypermodel_builder.build_model,\n",
        "                        objective='val_loss',\n",
        "                        max_trials=10,\n",
        "                        executions_per_trial=1, # The documentation says that makes things faster\n",
        "                        overwrite=True,\n",
        "                        directory='tuner_results',\n",
        "                        project_name='regression_tuning')\n",
        "\n",
        "# Summarize the search space\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MILTGt3fBQUq",
        "outputId": "b9de7041-ce68-4f8b-e800-f5a7335f149a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "model_name (Choice)\n",
            "{'default': 'GRU', 'conditions': [], 'values': ['GRU'], 'ordered': False}\n",
            "embedding_dim (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "dropout (Float)\n",
            "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': 0.1, 'sampling': 'linear'}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0001, 1e-05], 'ordered': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training & Evaluating"
      ],
      "metadata": {
        "id": "gUhGFFdL6RXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_accuracies(X_data, y_data, model):\n",
        "  # The predicted probabilities for each class\n",
        "  y_pred_probs = model.predict(X_data)\n",
        "\n",
        "  # The corresponding predicted labels\n",
        "  y_pred_labels = np.argmax(y_pred_probs, axis=0)\n",
        "\n",
        "  # The corresponding true labels\n",
        "  y_true_labels = np.argmax(y_data, axis=0)\n",
        "\n",
        "  # Compute confusion matrix\n",
        "  cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "  # Compute accuracies and weights for each class\n",
        "  class_accuracies = []\n",
        "  for class_label in range(len(cm)):\n",
        "      class_accuracy = cm[class_label, class_label] / np.sum(cm[class_label, :])\n",
        "      class_accuracies.append(class_accuracy)\n",
        "\n",
        "  # Print the classification report\n",
        "  print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "  return class_accuracies"
      ],
      "metadata": {
        "id": "5j5A0kXPINZH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=SEED)"
      ],
      "metadata": {
        "id": "jEF0i1-LPLu4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN_BASELINE:\n",
        "  baseline_model.fit(X_train, y_train)\n",
        "  joblib.dump(baseline_model, '/content/DL_Ass3/baseline_model_cla.pkl')\n",
        "else:\n",
        "  baseline_model = joblib.load('/content/DL_Ass3/baseline_model_cla.pkl')"
      ],
      "metadata": {
        "id": "loJTWWAG8o0N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = baseline_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "  # Print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESdu4ceVBRoz",
        "outputId": "0f1024d6-d42a-4be4-b196-589df72b59e7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9666657841086606\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98     36513\n",
            "           1       0.29      0.00      0.00      1256\n",
            "\n",
            "    accuracy                           0.97     37769\n",
            "   macro avg       0.63      0.50      0.49     37769\n",
            "weighted avg       0.94      0.97      0.95     37769\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model is only predicting 0"
      ],
      "metadata": {
        "id": "Gmfees9IJ-HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train or model"
      ],
      "metadata": {
        "id": "ft2Bh-bGBYZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True,  random_state=SEED)"
      ],
      "metadata": {
        "id": "ymP6xQgl8oKm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "0F-IEdInNB6V",
        "outputId": "c7a746e1-fff1-4dd3-b51b-764070cf0182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120859, 1600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN_MODEL:\n",
        "  # Early stop to prevent overfitting\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=4,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0,\n",
        "  )\n",
        "\n",
        "  # Perform hyperparameter search\n",
        "  tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=512, callbacks=[stop_early])\n",
        "  # Get the best hyperparameters\n",
        "  best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  # Build the model\n",
        "  best_model = tuner.hypermodel.build(best_hps)\n",
        "  # Save the model\n",
        "  best_model.save('content/DL_Ass3/best_model_cla.keras')\n",
        "  # Fit the model\n",
        "  history = best_model.fit(X_train, y_train, epochs=50, batch_size=512, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
        "  # Save the model\n",
        "  best_model.save('content/DL_Ass3/best_model_cla.keras')\n",
        "  plot_history(history)\n",
        "else:\n",
        "  best_model = keras.models.load_model('content/DL_Ass3/best_model_cla.keras')\n"
      ],
      "metadata": {
        "id": "UlLEXS2wBVu_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4cdbe8b-3320-41ba-f499-b511c0fb63d3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 00m 01s]\n",
            "\n",
            "Best val_loss So Far: None\n",
            "Total elapsed time: 00h 00m 02s\n",
            "\n",
            "Search: Running Trial #3\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "GRU               |GRU               |model_name\n",
            "128               |128               |embedding_dim\n",
            "96                |192               |units\n",
            "0                 |0.2               |dropout\n",
            "0.0001            |0.0001            |learning_rate\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\", line 113, in get_data_adapter\n",
            "    raise ValueError(f\"Unrecognized data type: x={x} (of type {type(x)})\")\n",
            "ValueError: Unrecognized data type: x=[[   0    0    0 ...    0    0 4333]\n",
            " [   0    0    0 ...   19  698  114]\n",
            " [   0    0    0 ...   14   33  288]\n",
            " ...\n",
            " [   0    0    0 ...    0   11    1]\n",
            " [   0    0    0 ...   85  137   13]\n",
            " [   0    0    0 ...    0    0    0]] (of type <class 'numpy.ndarray'>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\", line 113, in get_data_adapter\n    raise ValueError(f\"Unrecognized data type: x={x} (of type {type(x)})\")\nValueError: Unrecognized data type: x=[[   0    0    0 ...    0    0 4333]\n [   0    0    0 ...   19  698  114]\n [   0    0    0 ...   14   33  288]\n ...\n [   0    0    0 ...    0   11    1]\n [   0    0    0 ...   85  137   13]\n [   0    0    0 ...    0    0    0]] (of type <class 'numpy.ndarray'>)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-64b8bb82c3e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Perform hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstop_early\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Get the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mbest_hps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/__init__.py\", line 113, in get_data_adapter\n    raise ValueError(f\"Unrecognized data type: x={x} (of type {type(x)})\")\nValueError: Unrecognized data type: x=[[   0    0    0 ...    0    0 4333]\n [   0    0    0 ...   19  698  114]\n [   0    0    0 ...   14   33  288]\n ...\n [   0    0    0 ...    0   11    1]\n [   0    0    0 ...   85  137   13]\n [   0    0    0 ...    0    0    0]] (of type <class 'numpy.ndarray'>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4kA0W1xBUUC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}