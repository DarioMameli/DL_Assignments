{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "doAgp1etpKLh",
        "GpKMiHBHwxJf"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarioMameli/DL_Assignments/blob/main/DL_Assignment3_Clasification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DL Assignment 3\n",
        "## Group 4\n",
        "**AUTHORS**:\n",
        "*   Dario Mameli [dario.mameli@ugent.be]\n",
        "*   Àngel Masip LLopis [angel.masipllopis@ugent.be]\n",
        "*   Michele Russo [michele.russo@ugent.be]\n",
        "\n",
        "\n",
        "\n",
        "This notebook is to be intended as both report and code.\n",
        "\n",
        "Running on google colab is suggested to make sure all required libraries and packages are present."
      ],
      "metadata": {
        "id": "FRPrCmqA3ZTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GitHub\n",
        "\n",
        "Let's clone the repo with the dataset"
      ],
      "metadata": {
        "id": "MIZ_-9-u6E7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dkdk22/DL_Ass3\n",
        "import sys\n",
        "# Add the repository to the path\n",
        "sys.path.insert(1, '/content/DL_Ass3/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz0pecYrli1O",
        "outputId": "3d3d2eba-8a4f-4835-dafa-2ccf12e89b46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DL_Ass3'...\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 13 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (13/13), 30.71 MiB | 15.82 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzIrHK0HnwaT"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Firstly let's setup the workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Installs"
      ],
      "metadata": {
        "id": "VxmIWEgYIL00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of TensorFlow, which includes Keras (tf.keras)\n",
        "#!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade --quiet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install PrettyTable"
      ],
      "metadata": {
        "id": "gRSbKIjfILkE",
        "outputId": "427ef8b9-8838-49a1-d8b9-b85c7184acb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.9.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Requirement already satisfied: PrettyTable in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from PrettyTable) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiSBqqDaizpT"
      },
      "source": [
        "## Packages\n",
        "\n",
        "Let's import all the necessary functions and packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_tndfUaBvYFN"
      },
      "outputs": [],
      "source": [
        "from dl_utils import plot_history\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import keras\n",
        "from keras import regularizers\n",
        "import matplotlib.gridspec as gridspec\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import keras_tuner as kt\n",
        "from keras import backend as backend\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Conv2DTranspose,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    GlobalAveragePooling2D,\n",
        "    MaxPooling2D,\n",
        "    UpSampling2D\n",
        ")\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import json\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "from keras import Model, ops\n",
        "import joblib\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from nltk.stem import PorterStemmer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import keras.applications as app\n",
        "from keras.layers import LSTM, Dense, Embedding, Input, GRU, Bidirectional\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gc\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "import zipfile\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Global variables"
      ],
      "metadata": {
        "id": "zg_oAPilI2CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To accommodate computational constraints, we have opted to train the model and host the resulting datasets on GitHub. These global variables serve the purpose of indicating whether training or dataset creation is necessary, or if the pre-existing dataset from GitHub should be utilized."
      ],
      "metadata": {
        "id": "g9SlY87jPpgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED =42\n",
        "TRAIN_BASELINE = False\n",
        "TRAIN_MODEL = False\n",
        "EMBEDDINGS_GENERAL=False"
      ],
      "metadata": {
        "id": "PCYSJedrI4nW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsTb0qM5n7gE"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwus8LWjP33"
      },
      "source": [
        "Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'DL_Ass3/review_553850.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: #Extract the file\n",
        "    zip_ref.extractall()\n",
        "\n",
        "with open('review_553850.json', 'r') as json_file: # Open the json file\n",
        "    data = json.load(json_file)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for id, review in data[\"reviews\"].items():\n",
        "    review_text = review[\"review\"]\n",
        "    funny_votes = review[\"votes_funny\"]\n",
        "    X.append(review_text)\n",
        "    y.append(funny_votes)"
      ],
      "metadata": {
        "id": "pjelOlrluxfZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "dGl0DJoAmnus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to do the same preprocessing as in the other notebook. We are going to eliminate stopwords and also we are going to use steeming, tokenize and pad the sentences.\n"
      ],
      "metadata": {
        "id": "-NEwlSBGwIrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if EMBEDDINGS_GENERAL:\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = PorterStemmer()\n",
        "  # Remove stopwords, convert to lowercase and stemming\n",
        "  X_prepro = []\n",
        "  for text in X:\n",
        "      words = text.split()\n",
        "\n",
        "      filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "      stemmed_tokens = [stemmer.stem(word) for word in filtered_words]\n",
        "      preprocessed_text = ' '.join(stemmed_tokens)\n",
        "      X_prepro.append(preprocessed_text)\n",
        "  # Remove the reviews with 0 words\n",
        "  X_filtered = []\n",
        "  y_filtered = []\n",
        "  for review, label in zip(X_prepro, y):\n",
        "      if len(review.split()) > 0:\n",
        "          X_filtered.append(review)\n",
        "          y_filtered.append(label)\n",
        "  print(\"Length of the dataset: \"+str(len(X_filtered)))\n",
        "  # Tokenize\n",
        "  num_words=10000\n",
        "  tokenizer = Tokenizer(num_words=num_words)\n",
        "  tokenizer.fit_on_texts(X_filtered)\n",
        "  X_sequences = tokenizer.texts_to_sequences(X_filtered)\n",
        "  # Initialize variables to store information about the longest sequence\n",
        "  max_length = 0\n",
        "  max_index = -1\n",
        "\n",
        "  # Iterate through the list of sequences to have the longes sequence\n",
        "  for i, sequence in enumerate(X_sequences):\n",
        "      length = len(sequence)\n",
        "      # Update max_length and max_index if the current sequence is longer\n",
        "      if length > max_length:\n",
        "          max_length = length\n",
        "          max_index = i\n",
        "\n",
        "  # Retrieve the longest sequence\n",
        "  longest_sequence = X_sequences[max_index]\n",
        "\n",
        "  # Print the longest sequence\n",
        "  print(\"The longest sequence:\")\n",
        "  print(longest_sequence)\n",
        "  print(len(longest_sequence))\n",
        "  # Padding\n",
        "  max_sequence_length = len(longest_sequence)\n",
        "  padded_sequences = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
        "  # Convert to np arrays\n",
        "  scaler = StandardScaler()\n",
        "  normalized_data = scaler.fit_transform(padded_sequences)\n",
        "  X = np.array(normalized_data)\n",
        "  print(X.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "1KO92gjnn0sL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if EMBEDDINGS_GENERAL:\n",
        "  del stop_words\n",
        "  del stemmer\n",
        "  del X_prepro\n",
        "  del text\n",
        "  del words\n",
        "  del filtered_words\n",
        "  del stemmed_tokens\n",
        "  del preprocessed_text\n",
        "\n",
        "  del review\n",
        "  del num_words\n",
        "  del tokenizer\n",
        "  del X_sequences\n",
        "  del max_length\n",
        "  del max_index\n",
        "  del sequence\n",
        "  del longest_sequence\n",
        "  del padded_sequences\n",
        "  del scaler\n",
        "  del normalized_data\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "AeHIJivNDOTs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Binary classification problem"
      ],
      "metadata": {
        "id": "6GLrF8Ncrkik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this is a really difficoult task to learn because most of the reviews have 0 funny_votes and some of them have a really high value of funny_votes. That's why we are going to change the approach and convert the problem to a binary classification problem, that should be easier to learn to our model."
      ],
      "metadata": {
        "id": "avBowkvxrqPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to convert it by fixing an number of funny_votes N, treating a review as funny or not funny depending on whether it has more or fewer votes than N. In our case N is going to be 1 because of the unbalancement of the dataset."
      ],
      "metadata": {
        "id": "R0df2-SXsKod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = [0 if num == 0 else 1 for num in y_filtered]\n",
        "y = np.array(y)\n",
        "counts = [0,0]\n",
        "counts[0] = np.sum(data == 0)\n",
        "counts[1] = np.sum(data == 1)\n",
        "del y_filtered\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "WjWCPIAcQc1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d2089eb1-a444-4109-a310-6818d48d3f63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_filtered' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-cf0bc2b604cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_filtered\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_filtered' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating an embedd"
      ],
      "metadata": {
        "id": "TQnfpbZ746kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msJBo1iy5Fcj",
        "outputId": "4331cce8-178f-402a-c7d6-5c96f7081479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and\n",
        "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "BILsnXDu5GWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8fZFy6s49zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Baseline Model"
      ],
      "metadata": {
        "id": "ATIPmZsZNe9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "6madG_8ENiPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Best model for classification problem"
      ],
      "metadata": {
        "id": "zLZVRqSVBapQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperModelBuilderBinary(kt.HyperModel):\n",
        "    def __init__(self, max_sequence_length, loss_function, max_features):\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.loss_function = loss_function\n",
        "        self.max_features = max_features\n",
        "\n",
        "    def build_lstm_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(self.max_sequence_length,)))\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))  #fix embedding size?\n",
        "        model.add(LSTM(units=hp.Int('units', min_value=64, max_value=256, step=32), #https://keras.io/api/layers/recurrent_layers/lstm/\n",
        "                       dropout=hp.Float('dropout', min_value=0.2, max_value=0.4, step=0.2),\n",
        "                       recurrent_dropout=hp.Float('dropout', min_value=0.2, max_value=0.4, step=0.2), #reduce two dropouts\n",
        "                       input_shape=(self.max_sequence_length, 1)))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "    def build_gru_model(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(self.max_sequence_length,)))\n",
        "        model.add(Embedding(input_dim=self.max_features, output_dim=hp.Int('embedding_dim', min_value=128, max_value=128, step=32)))\n",
        "        model.add(GRU(units=hp.Int('units', min_value=64, max_value=256, step=32),   #https://keras.io/api/layers/recurrent_layers/gru/\n",
        "                      dropout=hp.Float('dropout', min_value=0.1, max_value=0.4, step=0.1),\n",
        "                      recurrent_dropout=hp.Float('dropout', min_value=0.1, max_value=0.4, step=0.1),\n",
        "                      input_shape=(self.max_sequence_length, 1)))\n",
        "        model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
        "                      loss=self.loss_function,\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "    def build_model(self,hp):\n",
        "        model_name = hp.Choice('model_name', values=['GRU', 'LSTM'])\n",
        "        if model_name == 'LSTM':\n",
        "            return self.build_lstm_model(hp)\n",
        "        elif model_name == 'GRU':\n",
        "            return self.build_gru_model(hp)\n",
        "\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        weight_for_0 = 1. / counts[0]\n",
        "        weight_for_1 = 1. / counts[1]\n",
        "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "        return model.fit(X_train, y_train, validation_data=(X_val, y_val),class_weight=class_weight)\n",
        "\n"
      ],
      "metadata": {
        "id": "UeCOUYOMshVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assuming y is a numpy array containing labels\n",
        "#and X is a numpy array containing features\n",
        "#Indices where y equals 0\n",
        "indices_y0 = np.where(y == 0)[0]\n",
        "size_X_y0 = len(indices_y0)\n",
        "\n",
        "#Indices where y equals 1\n",
        "indices_y1 = np.where(y == 1)[0]\n",
        "size_X_y1 = len(indices_y1)\n",
        "\n",
        "print(\"Size of X when y = 0:\", size_X_y0)\n",
        "print(\"Size of X when y = 1:\", size_X_y1)"
      ],
      "metadata": {
        "id": "wVN-PeTt2o-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss = keras.losses.BinaryCrossentropy(\n",
        "\n",
        "    name='binary_crossentropy'\n",
        ")\n",
        "# Define hyperparameters for the tuner\n",
        "hypermodel_builder = HyperModelBuilderBinary(max_sequence_length=max_sequence_length,\n",
        "                                       loss_function=loss, max_features=10000)\n",
        "\n",
        "# Initialize tuner\n",
        "tuner = kt.BayesianOptimization(hypermodel_builder.build_model,\n",
        "                        objective='val_loss',\n",
        "                        max_trials=1,\n",
        "                        executions_per_trial=1, # The documentation says that makes things faster\n",
        "                        overwrite=True,\n",
        "                        max_consecutive_failed_trials=5,\n",
        "                        directory='tuner_results',\n",
        "                        project_name='regression_tuning')\n",
        "\n",
        "# Summarize the search space\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "id": "MILTGt3fBQUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training & Evaluating"
      ],
      "metadata": {
        "id": "gUhGFFdL6RXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_accuracies(X_data, y_data, model):\n",
        "  # The predicted probabilities for each class\n",
        "  y_pred_probs = model.predict(X_data)\n",
        "\n",
        "  # The corresponding predicted labels\n",
        "  y_pred_labels = np.argmax(y_pred_probs, axis=0)\n",
        "\n",
        "  # The corresponding true labels\n",
        "  y_true_labels = np.argmax(y_data, axis=0)\n",
        "\n",
        "  # Compute confusion matrix\n",
        "  cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "  # Compute accuracies and weights for each class\n",
        "  class_accuracies = []\n",
        "  for class_label in range(len(cm)):\n",
        "      class_accuracy = cm[class_label, class_label] / np.sum(cm[class_label, :])\n",
        "      class_accuracies.append(class_accuracy)\n",
        "\n",
        "  # Print the classification report\n",
        "  print(classification_report(y_true_labels, y_pred_labels))\n",
        "\n",
        "  return class_accuracies"
      ],
      "metadata": {
        "id": "5j5A0kXPINZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def balance_classes(X_train, y_train):\n",
        "    # Find indices of examples for each class\n",
        "    indices_class_0 = np.where(y_train == 0)[0]\n",
        "    indices_class_1 = np.where(y_train == 1)[0]\n",
        "\n",
        "    # Calculate the number of examples in each class\n",
        "    num_class_0 = len(indices_class_0)\n",
        "    num_class_1 = len(indices_class_1)\n",
        "\n",
        "    # Randomly select a subset of examples from class 0 to match the number of examples in class 1\n",
        "    if num_class_0 > num_class_1:\n",
        "        # Randomly shuffle indices of class 0 examples\n",
        "        np.random.shuffle(indices_class_0)\n",
        "        # Keep only enough examples from class 0 to match the number of examples in class 1\n",
        "        indices_class_0 = indices_class_0[:num_class_1]\n",
        "    elif num_class_1 > num_class_0:\n",
        "        # Randomly shuffle indices of class 1 examples\n",
        "        np.random.shuffle(indices_class_1)\n",
        "        # Keep only enough examples from class 1 to match the number of examples in class 0\n",
        "        indices_class_1 = indices_class_1[:num_class_0]\n",
        "\n",
        "    # Combine the balanced indices\n",
        "    balanced_indices = np.concatenate([indices_class_0, indices_class_1])\n",
        "\n",
        "    # Shuffle the combined indices\n",
        "    np.random.shuffle(balanced_indices)\n",
        "\n",
        "    # Return the balanced subset of data\n",
        "    return X_train[balanced_indices], y_train[balanced_indices]\n"
      ],
      "metadata": {
        "id": "k_VXAZk031a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#balanced_X_train, balanced_y_train = balance_classes(X, y)\n",
        "X, _, y, _ = train_test_split(X_filtered,y, test_size=0.8, shuffle=True, random_state=SEED, stratify=y)"
      ],
      "metadata": {
        "id": "Pqcn-8PL4gso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jEF0i1-LPLu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69lZveHC6jEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create the embeddings"
      ],
      "metadata": {
        "id": "_RvApl765xT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "if EMBEDDINGS_GENERAL:\n",
        "  # Load the BERT model\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  # Define a function to encode a sentence using the model\n",
        "  def encode_sentence(sentence):\n",
        "      return model.encode(sentence)\n",
        "\n",
        "  # Number of GPUs to use\n",
        "  num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "  # Check if GPU is available\n",
        "  if num_gpus == 0:\n",
        "      print(\"No GPU available. Using CPU.\")\n",
        "  else:\n",
        "      print(\"Number of GPUs available:\", num_gpus)\n",
        "\n",
        "  # Distribute the work across multiple GPUs using MirroredStrategy\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "\n",
        "  # Encode sentences using the model in parallel\n",
        "  encoded_sentences = []\n",
        "  i=0\n",
        "  for sentence in X:\n",
        "      i=i+1\n",
        "      encoded_sentence = encode_sentence(sentence)\n",
        "      encoded_sentences.append(encoded_sentence)\n",
        "      print(i)\n",
        "\n",
        "  # Convert the list of encoded sentences into a numpy array\n",
        "  X = np.array(encoded_sentences)\n",
        "\n",
        "  # Print the shape of the encoded sentences\n",
        "  print(\"Shape of encoded sentences:\", X.shape)\n",
        "  # Assuming X_train is your encoded dataset\n",
        "  np.save('/content/X.npy', X)\n",
        "else:\n",
        "  X = np.load('/content/X.npy')\n"
      ],
      "metadata": {
        "id": "8BWMvBo551lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, shuffle=True, random_state=SEED, stratify=y)"
      ],
      "metadata": {
        "id": "Ef5eNZak_a74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "count_1=0\n",
        "count_0=0\n",
        "for sample in y_train:\n",
        "  if sample==0:\n",
        "    count_0=1+count_0\n",
        "  else:\n",
        "    count_1=1+count_1\n",
        "print(\"class 1 \", count_1)\n",
        "print(\"class 0\", count_0)"
      ],
      "metadata": {
        "id": "Wu8hoEGDzyba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if TRAIN_BASELINE:\n",
        "baseline_model.fit(X_train, y_train)\n",
        "joblib.dump(baseline_model, '/content/DL_Ass3/baseline_model_cla.pkl')\n",
        "#else:\n",
        "baseline_model = joblib.load('/content/DL_Ass3/baseline_model_cla.pkl')"
      ],
      "metadata": {
        "id": "loJTWWAG8o0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = baseline_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "  # Print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ESdu4ceVBRoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the model is only predicting 0"
      ],
      "metadata": {
        "id": "Gmfees9IJ-HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train or model"
      ],
      "metadata": {
        "id": "ft2Bh-bGBYZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True,  random_state=SEED, stratify=y_train)"
      ],
      "metadata": {
        "id": "ymP6xQgl8oKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del y\n",
        "del X\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "fQbvPm9lD-Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "if TRAIN_MODEL:\n",
        "  # Early stop to prevent overfitting\n",
        "  stop_early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0,\n",
        "    patience=4,\n",
        "    verbose=0,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=0,\n",
        "  )\n",
        "\n",
        "  # Perform hyperparameter search\n",
        "  tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=1, batch_size=256, callbacks=[stop_early])\n",
        "  # Get the best hyperparameters\n",
        "  best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  # Define the file name where you want to save the hyperparameters\n",
        "  file_name = \"/content/DL_Ass3/best_hps_cla.json\"\n",
        "  #Save the hp\n",
        "  with open(\"/content/DL_Ass3/best_hps_cla.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_hps, f)\n",
        "  # Build the model\n",
        "  best_model = tuner.hypermodel.build(best_hps)\n",
        "  print(best_model)\n",
        "  # Fit the model\n",
        "  history = best_model.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
        "  # Save the model\n",
        "  best_model.save_weights('/content/DL_Ass3/best_model_cla.weights.h5')\n",
        "  plot_history(history)\n",
        "else:\n",
        "  with open(\"/content/DL_Ass3/best_hps_cla.pkl\", \"wb\") as f:\n",
        "    best_hps = pickle.load(f)\n",
        "  best_model = tuner.hypermodel.build(best_hps)\n",
        "  best_model.load_weights('/content/DL_Ass3/best_model_cla.weights.h5')\n"
      ],
      "metadata": {
        "id": "UlLEXS2wBVu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/DL_Ass3/best_hps_cla.pkl\", \"rb\") as f:\n",
        "  best_hps = pickle.load(f)\n",
        "best_model = tuner.hypermodel.build(best_hps)"
      ],
      "metadata": {
        "id": "Oks9dzI-aLAI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}